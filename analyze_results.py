#!/usr/bin/env python3
"""
Script to analyze training results from .npy files generated by train.py
"""

import numpy as np
import sys
import os

def analyze_training_results(result_dir):
    """Analyze training results from a directory containing .npy files"""
    
    print(f"Analyzing results from: {result_dir}")
    print("=" * 60)
    
    try:
        # Load all the arrays
        steps = np.load(os.path.join(result_dir, 'steps.npy'))
        train_metrics = np.load(os.path.join(result_dir, 'all_train_metric.npy'))
        val_metrics = np.load(os.path.join(result_dir, 'all_val_metric.npy'))
        test_metric = np.load(os.path.join(result_dir, 'test_metric.npy'))
        times = np.load(os.path.join(result_dir, 'all_time.npy'))
        
        print("File shapes:")
        print(f"  Steps: {steps.shape}")
        print(f"  Train metrics: {train_metrics.shape}")
        print(f"  Validation metrics: {val_metrics.shape}")
        print(f"  Test metric: {test_metric.shape}")
        print(f"  Times: {times.shape}")
        print()
        
        # Extract key information
        total_steps = steps[-1] if len(steps) > 0 else 0
        best_val_metric = np.max(val_metrics)
        best_val_idx = np.argmax(val_metrics)
        final_train_metric = train_metrics[-1] if len(train_metrics) > 0 else 0
        final_val_metric = val_metrics[-1] if len(val_metrics) > 0 else 0
        
        # Handle test metric (might be scalar or array)
        if test_metric.ndim == 0:
            final_test_metric = float(test_metric)
        else:
            final_test_metric = test_metric[-1] if len(test_metric) > 0 else 0
        
        total_time = np.sum(times) if len(times) > 0 else 0
        
        print("Training Summary:")
        print(f"  Total steps completed: {total_steps}")
        print(f"  Total training time: {total_time:.2f} seconds ({total_time/60:.2f} minutes)")
        print()
        
        print("Performance Metrics:")
        print(f"  Best validation metric: {best_val_metric:.4f} (at step {steps[best_val_idx] if best_val_idx < len(steps) else 'N/A'})")
        print(f"  Final train metric: {final_train_metric:.4f}")
        print(f"  Final validation metric: {final_val_metric:.4f}")
        print(f"  Final test metric: {final_test_metric:.4f}")
        print()
        
        # Training progress analysis
        if len(val_metrics) > 1:
            val_improvement = val_metrics[-1] - val_metrics[0]
            train_improvement = train_metrics[-1] - train_metrics[0]
            
            print("Training Progress:")
            print(f"  Validation improvement: {val_improvement:+.4f}")
            print(f"  Training improvement: {train_improvement:+.4f}")
            
            # Check for overfitting
            if len(val_metrics) > 5:
                recent_val_trend = np.mean(val_metrics[-5:]) - np.mean(val_metrics[-10:-5])
                if recent_val_trend < -0.01:
                    print(f"  ⚠️  Possible overfitting detected (recent val trend: {recent_val_trend:+.4f})")
                else:
                    print(f"  ✓ Validation trend looks healthy (recent trend: {recent_val_trend:+.4f})")
        
        print()
        print("Files analyzed successfully!")
        return {
            'best_val': best_val_metric,
            'final_test': final_test_metric,
            'final_train': final_train_metric,
            'total_steps': total_steps,
            'total_time': total_time
        }
        
    except FileNotFoundError as e:
        print(f"Error: Could not find file {e.filename}")
        return None
    except Exception as e:
        print(f"Error analyzing results: {e}")
        return None

def main():
    if len(sys.argv) != 2:
        print("Usage: python analyze_results.py <results_directory>")
        sys.exit(1)
    
    result_dir = sys.argv[1]
    
    if not os.path.isdir(result_dir):
        print(f"Error: {result_dir} is not a valid directory")
        sys.exit(1)
    
    analyze_training_results(result_dir)

if __name__ == "__main__":
    main()
